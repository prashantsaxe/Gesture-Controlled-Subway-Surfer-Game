```markdown
# ğŸ–ï¸ Gesture Controlled Subway Surfer Game

Control **Subway Surfers** using **hand gestures in real-time** via webcam!

This project uses **MediaPipe, scikit-learn, OpenCV**, and machine learning models to recognize hand gestures and simulate keyboard presses for game actions such as moving left/right, jumping, rolling, using hoverboard, and pausing the game.

---

## ğŸ® Demo

[â–¶ï¸ Watch Demo Video](https://github.com/brij26/Gesture-Controlled-Subway-Surfer-Game/blob/main/output.mp4)

---

## ğŸš€ Features

âœ… Real-time gesture detection using webcam  
âœ… Machine learning models for gesture classification (RandomForest, LSTM, Transformer)  
âœ… Gesture-to-keyboard mapping using `pyautogui`  
âœ… Works with Subway Surfers (Web or PC version)  
âœ… Easily extendable for training your own gestures and actions

---

## âœ‹ Recognized Gestures

| Gesture     | Action          | Key Pressed       |
|-------------|-----------------|-------------------|
| `go_left`   | Move Left       | â¬…ï¸ `a`            |
| `go_right`  | Move Right      | â¡ï¸ `d`            |
| `jump`      | Jump            | â¬†ï¸ `w`            |
| `roll`      | Roll            | â¬‡ï¸ `s`            |
| `hoverboard`| Use Hoverboard  | `space` (twice)   |
| `pause`     | Pause Game      | `p`               |
| `relax`     | No Action       | â€”                 |

---

## ğŸ§  How It Works

1. **Data Collection**  
   Collect hand gesture data using MediaPipe, saved as `.npy` files under `gesture_data/`.
   
2. **Model Training**  
   Train a machine learning model (`RandomForest`, `LSTM`, or `Transformer`) using `models/` scripts on the gesture data.

3. **Live Prediction**  
   Webcam captures your hand â†’ predicts gesture â†’ maps it to keyboard action to control Subway Surfers in real-time.

---

## ğŸ› ï¸ Tech Stack

- Python
- OpenCV
- MediaPipe
- scikit-learn
- pyautogui
- NumPy
- joblib

---

## ğŸ“‚ Directory Structure

```

prashantsaxe-gesture-controlled-subway-surfer-game/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data\_collection/
â”œâ”€â”€ detect\_only\_hand/
â”œâ”€â”€ find\_trails\_of\_hand\_points/
â”œâ”€â”€ gesture\_data/
â”œâ”€â”€ models/
â””â”€â”€ results/

````

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/<your-username>/prashantsaxe-gesture-controlled-subway-surfer-game.git
cd prashantsaxe-gesture-controlled-subway-surfer-game

# Create environment (recommended)
conda create -p venv python=3.10 -y
conda activate ./venv

# If conda is not available, install Miniconda or Anaconda first

# Install dependencies
pip install -r requirements.txt
````

---

## ğŸš€ Running

* **Collect Gesture Data:**

  ```bash
  python data_collection/data_collection_jump.py
  # Replace with the corresponding gesture script you want to record
  ```
* **Train Model:**

  ```bash
  python models/randomforest.py
  # or
  python models/train_LSTM.py
  # or
  python models/train_transformer.py
  ```
* **Run Live Gesture Control:**

  ```bash
  python results/result.py
  ```

---

## ğŸ“ˆ Contributions

Feel free to open issues or pull requests for improvements, new gesture integrations, or adding better models.

---

## ğŸ“œ License

This project is open-source for learning and personal projects. Feel free to modify and experiment.

---

Enjoy controlling Subway Surfers with your gestures! ğŸš€âœ‹ğŸ•¹ï¸

```

---

### How to use:

âœ… Simply **copy and paste** directly into your `prashantsaxe-gesture-controlled-subway-surfer-game/README.md`.  
âœ… Replace `<your-username>` with your GitHub username in the `git clone` URL before pushing.  
âœ… It is cleanly structured, readable, SEO-friendly, and showcases your project professionally on GitHub.  

If you need badges (stars, license, python version) or a minimal GIF preview for the top, let me know for instant addition before you push.
```
